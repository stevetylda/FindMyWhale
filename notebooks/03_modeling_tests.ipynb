{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54cc5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------- #\n",
    "#                              MODULES                              #\n",
    "\n",
    "# Standard Modules\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "\n",
    "# Third-Party Modules\n",
    "import h3\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from shapely.geometry import Polygon\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#                                                                   #\n",
    "# ----------------------------------------------------------------- #\n",
    "\n",
    "# ----------------------------------------------------------------- #\n",
    "#                             FUNCTIONS                             #\n",
    "\n",
    "##############\n",
    "# COLLECTION\n",
    "\n",
    "\n",
    "# Loan and Process Sightings\n",
    "def load_and_process_sighting_data(\n",
    "    directory: str,\n",
    "    date_col: str,\n",
    "    lat_col: str,\n",
    "    lon_col: str,\n",
    "    id_col: str,\n",
    "    h3_resolution: int,\n",
    "    start_date: str = None,\n",
    "    source: Literal[\"TMW\", \"ACARTIA\"] = \"TMW\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and process sighting data for TMW or Acartia.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to CSV files.\n",
    "        date_col (str): Column name containing datetime string.\n",
    "        lat_col (str): Latitude column name.\n",
    "        lon_col (str): Longitude column name.\n",
    "        id_col (str): Unique identifier or countable column.\n",
    "        h3_resolution (int): H3 resolution to use.\n",
    "        start_date (str, optional): Filter records to start at this date.\n",
    "        source (str): \"TMW\" or \"ACARTIA\", for minor formatting differences.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated sightings data with full date-grid coverage.\n",
    "    \"\"\"\n",
    "    # Read & concat all CSVs\n",
    "    data = pd.concat([pd.read_csv(path) for path in glob.glob(f\"{directory}/*.csv\")])\n",
    "    data.columns = data.columns.str.upper()\n",
    "\n",
    "    # Parse date and geo\n",
    "    data[\"DATE\"] = data[date_col].str[:10]\n",
    "    data[\"LATITUDE\"] = pd.to_numeric(data[lat_col], errors=\"coerce\")\n",
    "    data[\"LONGITUDE\"] = pd.to_numeric(data[lon_col], errors=\"coerce\")\n",
    "    data = data.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"])\n",
    "\n",
    "    # Calculate H3 grid\n",
    "    h3_col = f\"H3_GRID_{h3_resolution}\"\n",
    "    data[h3_col] = data.apply(\n",
    "        lambda x: h3.latlng_to_cell(x[\"LATITUDE\"], x[\"LONGITUDE\"], h3_resolution),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    data[\"DATE\"] = pd.to_datetime(data[\"DATE\"])\n",
    "    if start_date:\n",
    "        data = data[data[\"DATE\"] >= pd.to_datetime(start_date)]\n",
    "\n",
    "    # Aggregate sightings\n",
    "    data_agg = data.groupby([\"DATE\", h3_col], as_index=False).agg(\n",
    "        SIGHTING_COUNT=(id_col, \"count\")\n",
    "    )\n",
    "\n",
    "    # Build full date-grid frame\n",
    "    all_dates = pd.date_range(data[\"DATE\"].min(), data[\"DATE\"].max())\n",
    "    all_grids = data[h3_col].unique()\n",
    "    full_index = pd.MultiIndex.from_product(\n",
    "        [all_dates, all_grids], names=[\"DATE\", h3_col]\n",
    "    )\n",
    "    full_df = pd.DataFrame(index=full_index).reset_index()\n",
    "    full_df[\"DATE\"] = pd.to_datetime(full_df[\"DATE\"])\n",
    "\n",
    "    # Merge to ensure all date/grid combos exist\n",
    "    full_data = pd.merge(full_df, data_agg, on=[\"DATE\", h3_col], how=\"left\")\n",
    "    full_data[\"SIGHTING_COUNT\"] = full_data[\"SIGHTING_COUNT\"].fillna(0)\n",
    "\n",
    "    return full_data\n",
    "\n",
    "\n",
    "# H3 to Polygon Extent\n",
    "def h3_to_polygon(h3_index):\n",
    "    latlon = h3.cell_to_boundary(h3_index)\n",
    "    return Polygon([(lon, lat) for lat, lon in latlon])  # Note lon/lat flip\n",
    "\n",
    "\n",
    "# Clip Sightings to Geometry\n",
    "def clip_sightings_to_geometry(\n",
    "    sightings_df, h3_resolution, geometry_gdf, geometry_col=\"geometry\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Clips a sightings dataframe to a geometry (e.g., marine area) based on H3 grid resolution.\n",
    "\n",
    "    Parameters:\n",
    "    - sightings_df (pd.DataFrame): Your sightings data with H3 columns like 'H3_GRID_7'.\n",
    "    - h3_resolution (int): H3 resolution to use for clipping (e.g., 7).\n",
    "    - geometry_gdf (gpd.GeoDataFrame): GeoDataFrame containing geometry to clip against.\n",
    "    - geometry_col (str): The name of the geometry column in geometry_gdf (default: 'geometry').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The clipped sightings dataframe.\n",
    "    \"\"\"\n",
    "    h3_col = f\"H3_GRID_{h3_resolution}\"\n",
    "    tmp_df = sightings_df[[h3_col]].drop_duplicates().copy()\n",
    "    tmp_df[\"geometry\"] = tmp_df[h3_col].apply(h3_to_polygon)\n",
    "    tmp_df = gpd.GeoDataFrame(tmp_df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    # Clip to the first polygon (or adjust to match your context)\n",
    "    tmp_df = tmp_df.clip(geometry_gdf[geometry_col].iloc[0])\n",
    "\n",
    "    # Merge to keep only the sightings within the clipped geometry\n",
    "    clipped_sightings = pd.merge(sightings_df, tmp_df, on=h3_col)\n",
    "\n",
    "    return clipped_sightings\n",
    "\n",
    "\n",
    "#                                                                   #\n",
    "# ----------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11245d",
   "metadata": {},
   "source": [
    "### 1. Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0a53e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "## H3 Grid Resolution for Modeling\n",
    "h3_resolution = 4\n",
    "\n",
    "## TMW Data Path\n",
    "tmw_directory = \"/Users/tylerstevenson/Documents/CODE/orcasalmon/data/twm\"\n",
    "\n",
    "## Acartia Data Path\n",
    "acartia_directory = (\n",
    "    \"/Users/tylerstevenson/Documents/CODE/FindMyWhale/data/raw/sightings\"\n",
    ")\n",
    "\n",
    "## Marine Area Geometries\n",
    "marine_geometries_path = \"/Users/tylerstevenson/Documents/CODE/FindMyWhale/data/processed/GIS/POLYGONS/SSEA_REGION_5.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingest\n",
    "## Open Marine Geometries\n",
    "marine_area = gpd.read_parquet(marine_geometries_path)\n",
    "marine_area = marine_area.dissolve()\n",
    "\n",
    "## Open TMW\n",
    "tmw_data_cleaned = load_and_process_sighting_data(\n",
    "    directory=tmw_directory,\n",
    "    date_col=\"SIGHTDATE\",\n",
    "    lat_col=\"LATITUDE\",\n",
    "    lon_col=\"LONGITUDE\",\n",
    "    id_col=\"DATE\",  # or other proxy for sightings count\n",
    "    h3_resolution=h3_resolution,\n",
    "    source=\"TMW\",\n",
    ")\n",
    "\n",
    "## Open Acartia\n",
    "acartia_data_cleaned = load_and_process_sighting_data(\n",
    "    directory=acartia_directory,\n",
    "    date_col=\"CREATED\",\n",
    "    lat_col=\"LATITUDE\",\n",
    "    lon_col=\"LONGITUDE\",\n",
    "    id_col=\"ENTRY_ID\",\n",
    "    h3_resolution=h3_resolution,\n",
    "    start_date=\"2022-01-01\",\n",
    "    source=\"ACARTIA\",\n",
    ")\n",
    "\n",
    "# Conbine Sightings Data\n",
    "sightings_data_raw = pd.concat([acartia_data_cleaned, tmw_data_cleaned])\n",
    "\n",
    "## Clip to Marine Area\n",
    "sightings_data_raw = clip_sightings_to_geometry(\n",
    "    sightings_data_raw, h3_resolution=h3_resolution, geometry_gdf=marine_area\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229082d5",
   "metadata": {},
   "source": [
    "### 2. Feature Creation\n",
    "#### Preprocessing\n",
    "- Aggregate to Weekly Sightings\n",
    "- Convert Sightings to Ratio of Total Sightings\n",
    "\n",
    "#### Features\n",
    "- Definition Signals\n",
    "    - H3 Grid - Categorical Encoding\n",
    "    - H3 Parent Grid - Categorical Encoding\n",
    "    - Percent Grid Over Water\n",
    "- Temporal Signals\n",
    "    - Total Sightings\n",
    "    - Number of Days with Atleast One Sighting Per Week\n",
    "    - Add Week of Year\n",
    "    - Add Season\n",
    "    - Add Month\n",
    "    - Add Prior Week of Year\n",
    "    - Add Prior Season\n",
    "    - Add Prior Month\n",
    "    - Week Has Any Sighting Boolean Flag\n",
    "    - Is Holiday Week < need to define a custom whale-likely observation holidays >\n",
    "    - Is Holiday Week / Holiday Weekend < need to define a custom whale-likely observation holidays >\n",
    "    - Is School Break\n",
    "    - Lagged Sightings Ratio to Capture Near Term Auto-Correlation (Lag 1 - Lag n)\n",
    "    - Prescence Lagged Sightings Ratio to Capture Near Term Auto-Correlation (Lag 1 - Lag n) < sighting / no sighting in previous observation>\n",
    "    - Sighting Count Diff Over Lags\n",
    "    - Add Some Seasonal Lag Components (These seem to be important but need to check less common grids -> Lag Periods: 28, 29, 52, 56, 57, 113) \n",
    "    - Rolling Mean\n",
    "    - Rolling Std\n",
    "    - Cumulative Sightings Over Last N Weeks (N = 4)\n",
    "    - Relative Effort Index (weekly sightings relative to long-term weekly median)\n",
    "    - Capture Multi-Scale Dependencies\n",
    "        - Month Sin\n",
    "        - Month Cos\n",
    "        - Year Sin\n",
    "        - Year Cos\n",
    "    - Fourier Transform of Week, Month, Year\n",
    "    - Ratio of Weeks with Observation Over Prior N Weeks\n",
    "- Spatial Signals\n",
    "    - Neighbor Lagged Sightings\n",
    "    - Neighbor Prescence Lagged Sightings \n",
    "    - Has Active Neighbors\n",
    "    - Centroid Lat/Long\n",
    "- Data Lapse Check\n",
    "    - Data Transition Week (e.g. TWM -> Acartia)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c974d336",
   "metadata": {},
   "source": [
    "#### Feature Enrichment\n",
    "##### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "85d1115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import holidays\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "us_holidays = holidays.UnitedStates()\n",
    "\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"summer\"\n",
    "    else:\n",
    "        return \"fall\"\n",
    "\n",
    "\n",
    "def add_statistical_week(df, date_col=\"DATE\"):\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Set Sunday as start of week\n",
    "    df[\"WEEK_START\"] = df[date_col] - pd.to_timedelta(\n",
    "        df[date_col].dt.weekday + 1, unit=\"D\"\n",
    "    )\n",
    "    df[\"WEEK_START\"] = df[\"WEEK_START\"].dt.normalize()\n",
    "\n",
    "    df[\"WEEK_YEAR\"] = df[\"WEEK_START\"].dt.year\n",
    "    df[\"WEEK_NUMBER\"] = df[\"WEEK_START\"].dt.isocalendar().week\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_holiday(week_start):\n",
    "    week_dates = [week_start + timedelta(days=i) for i in range(7)]\n",
    "    return any(date in us_holidays for date in week_dates)\n",
    "\n",
    "\n",
    "def is_holiday_with_overlap(week_start):\n",
    "    week_dates = [week_start + timedelta(days=i) for i in range(7)]\n",
    "    for d in week_dates:\n",
    "        if d in us_holidays:\n",
    "            return True\n",
    "        if d.weekday() in [4, 5, 6]:  # Fri/Sat/Sun\n",
    "            for offset in [-1, 0, 1]:\n",
    "                if (d + timedelta(days=offset)) in us_holidays:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_school_break(week_start):\n",
    "    return week_start.month in [6, 7, 8]  # Rough guess, refine if needed\n",
    "\n",
    "\n",
    "def add_time_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"WEEK_OF_YEAR\"] = df[\"WEEK_START\"].dt.isocalendar().week\n",
    "    df[\"MONTH\"] = df[\"WEEK_START\"].dt.month\n",
    "    df[\"SEASON\"] = df[\"MONTH\"].apply(get_season)\n",
    "\n",
    "    # Prior periods\n",
    "    prior_dates = df[\"WEEK_START\"] - pd.Timedelta(days=7)\n",
    "    df[\"PRIOR_WEEK_OF_YEAR\"] = prior_dates.dt.isocalendar().week\n",
    "    df[\"PRIOR_MONTH\"] = prior_dates.dt.month\n",
    "    df[\"PRIOR_SEASON\"] = df[\"PRIOR_MONTH\"].apply(get_season)\n",
    "\n",
    "    # Holiday and seasonal break flags\n",
    "    df[\"IS_HOLIDAY\"] = df[\"WEEK_START\"].apply(is_holiday).astype(int)\n",
    "    df[\"IS_HOLIDAY_OVERLAP\"] = (\n",
    "        df[\"WEEK_START\"].apply(is_holiday_with_overlap).astype(int)\n",
    "    )\n",
    "    df[\"IS_SCHOOL_BREAK\"] = df[\"WEEK_START\"].apply(is_school_break).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_weekly_sightings(sightings_data, h3_resolution):\n",
    "    \"\"\"\n",
    "    Processes sightings data into a weekly, grid-based format with relative ratios, temporal features,\n",
    "    and H3 spatial context.\n",
    "\n",
    "    Parameters:\n",
    "    - sightings_data (pd.DataFrame): Raw sightings data with 'DATE' and H3 column at provided resolution.\n",
    "    - h3_resolution (int): The resolution of the H3 grid column to use.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed and aggregated sightings data.\n",
    "    \"\"\"\n",
    "    h3_col = f\"H3_GRID_{h3_resolution}\"\n",
    "\n",
    "    # 1. Add Statistical Week\n",
    "    sightings_data = add_statistical_week(df=sightings_data, date_col=\"DATE\")\n",
    "\n",
    "    # 2. Boolean flag for any sightings per day\n",
    "    sightings_data[\"SIGHTING_BOOL\"] = (sightings_data[\"SIGHTING_COUNT\"] > 0).astype(int)\n",
    "    sightings_day_bool = sightings_data.groupby(\n",
    "        [\"WEEK_START\", h3_col], as_index=False\n",
    "    ).agg(N_DAYS_WITH_OBS=(\"SIGHTING_BOOL\", \"sum\"))\n",
    "\n",
    "    # 3. Convert to weekly count per H3 cell\n",
    "    sightings_data = sightings_data.groupby([\"WEEK_START\", h3_col], as_index=False)[\n",
    "        \"SIGHTING_COUNT\"\n",
    "    ].sum()\n",
    "\n",
    "    # 4. Total sightings across all grids per week\n",
    "    sightings_weekly_total = sightings_data.groupby(\"WEEK_START\", as_index=False).agg(\n",
    "        TOTAL_SIGHTING_COUNT=(\"SIGHTING_COUNT\", \"sum\")\n",
    "    )\n",
    "\n",
    "    # 5. Merge total sightings into main df\n",
    "    sightings_data = pd.merge(\n",
    "        sightings_data, sightings_weekly_total, on=\"WEEK_START\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 6. Merge in daily obs flag per cell-week\n",
    "    sightings_data = pd.merge(\n",
    "        sightings_data, sightings_day_bool, on=[\"WEEK_START\", h3_col], how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 7. Relative Sighting Ratio\n",
    "    sightings_data[\"SIGHTING_RATIO\"] = (\n",
    "        sightings_data[\"SIGHTING_COUNT\"] / sightings_data[\"TOTAL_SIGHTING_COUNT\"]\n",
    "    )\n",
    "\n",
    "    # 8. Add time-based features\n",
    "    sightings_data = add_time_features(sightings_data)\n",
    "\n",
    "    # 9. Weekly flag: did any grid see a sighting this week?\n",
    "    weekly_sighting_flag = (\n",
    "        sightings_data.groupby(\"WEEK_START\")[\"SIGHTING_COUNT\"].sum().reset_index()\n",
    "    )\n",
    "    weekly_sighting_flag[\"WEEK_HAS_ANY_SIGHTING\"] = (\n",
    "        weekly_sighting_flag[\"SIGHTING_COUNT\"] > 0\n",
    "    ).astype(int)\n",
    "    weekly_sighting_flag = weekly_sighting_flag[[\"WEEK_START\", \"WEEK_HAS_ANY_SIGHTING\"]]\n",
    "    sightings_data = sightings_data.merge(\n",
    "        weekly_sighting_flag, on=\"WEEK_START\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 10. Add H3 centroid lat/lon\n",
    "    sightings_data[[\"CENTROID_LAT\", \"CENTROID_LON\"]] = sightings_data[h3_col].apply(\n",
    "        lambda h: pd.Series(h3.cell_to_latlng(h))\n",
    "    )\n",
    "\n",
    "    # 11. Add H3 parent cells (res 1â€“3)\n",
    "    for res in [1, 2, 3]:\n",
    "        parent_col = f\"H3_GRID_PARENT_{res}\"\n",
    "        sightings_data[parent_col] = sightings_data[h3_col].apply(\n",
    "            lambda h: h3.cell_to_parent(h, res)\n",
    "        )\n",
    "\n",
    "    return sightings_data\n",
    "\n",
    "\n",
    "def add_lag_features_ratio_base(\n",
    "    df,\n",
    "    h3_col,\n",
    "    date_col=\"WEEK_START\",\n",
    "    sighting_ratio_col=\"SIGHTING_RATIO\",\n",
    "    max_lag=5,\n",
    "    seasonal_lags=[28, 29, 52, 56, 57, 113],\n",
    "    rolling_windows=[3, 5, 7],\n",
    "    cum_weeks=4,\n",
    "    obs_flag_col=None,\n",
    "    prior_weeks_obs_ratio_window=12,\n",
    "):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values([h3_col, date_col])\n",
    "\n",
    "    # If presence bool flag not provided, create from ratio > 0\n",
    "    if obs_flag_col is None:\n",
    "        df[\"presence_bool\"] = (df[sighting_ratio_col] > 0).astype(int)\n",
    "        obs_flag_col = \"presence_bool\"\n",
    "\n",
    "    # Lagged sightings ratio\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        df[f\"lag_ratio_{lag}\"] = df.groupby(h3_col)[sighting_ratio_col].shift(lag)\n",
    "        df[f\"presence_lag_{lag}\"] = df.groupby(h3_col)[obs_flag_col].shift(lag)\n",
    "\n",
    "    # Lag diff (lag_ratio_1 - lag_ratio_2)\n",
    "    df[\"lag_diff_1_2\"] = df[\"lag_ratio_1\"] - df[\"lag_ratio_2\"]\n",
    "\n",
    "    # Seasonal lags on sighting ratio\n",
    "    for lag in seasonal_lags:\n",
    "        df[f\"seasonal_lag_{lag}\"] = df.groupby(h3_col)[sighting_ratio_col].shift(lag)\n",
    "\n",
    "    # Rolling mean and std of sighting ratio\n",
    "    for window in rolling_windows:\n",
    "        df[f\"rolling_mean_{window}\"] = df.groupby(h3_col)[sighting_ratio_col].transform(\n",
    "            lambda x: x.shift(1).rolling(window).mean()\n",
    "        )\n",
    "        df[f\"rolling_std_{window}\"] = df.groupby(h3_col)[sighting_ratio_col].transform(\n",
    "            lambda x: x.shift(1).rolling(window).std()\n",
    "        )\n",
    "\n",
    "    # Cumulative sum over last N weeks of ratio\n",
    "    df[f\"cumsum_{cum_weeks}\"] = df.groupby(h3_col)[sighting_ratio_col].transform(\n",
    "        lambda x: x.shift(1).rolling(cum_weeks).sum()\n",
    "    )\n",
    "\n",
    "    # Relative Effort Index: weekly ratio relative to long-term median (per H3)\n",
    "    median_ratio = df.groupby(h3_col)[sighting_ratio_col].transform(\"median\")\n",
    "    df[\"relative_effort_index\"] = df[sighting_ratio_col] / (\n",
    "        median_ratio.replace(0, np.nan)\n",
    "    )\n",
    "    df[\"relative_effort_index\"] = df[\"relative_effort_index\"].fillna(0)\n",
    "\n",
    "    # Cyclic month & year features\n",
    "    df[\"month\"] = df[date_col].dt.month\n",
    "    df[\"year\"] = df[date_col].dt.year\n",
    "\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "\n",
    "    year_min, year_max = df[\"year\"].min(), df[\"year\"].max()\n",
    "    year_scaled = (df[\"year\"] - year_min) / (year_max - year_min + 1e-9)\n",
    "    df[\"year_sin\"] = np.sin(2 * np.pi * year_scaled)\n",
    "    df[\"year_cos\"] = np.cos(2 * np.pi * year_scaled)\n",
    "\n",
    "    # Fourier features for week, month, year\n",
    "    def fourier_feats(series, n_harmonics=3):\n",
    "        feats = {}\n",
    "        for k in range(1, n_harmonics + 1):\n",
    "            feats[f\"fourier_sin_{k}\"] = np.sin(2 * np.pi * k * series)\n",
    "            feats[f\"fourier_cos_{k}\"] = np.cos(2 * np.pi * k * series)\n",
    "        return pd.DataFrame(feats)\n",
    "\n",
    "    df[\"week_of_year\"] = df[date_col].dt.isocalendar().week.astype(int)\n",
    "    df = pd.concat([df, fourier_feats(df[\"week_of_year\"])], axis=1)\n",
    "    df = pd.concat([df, fourier_feats(df[\"month\"])], axis=1)\n",
    "    df = pd.concat([df, fourier_feats(year_scaled)], axis=1)\n",
    "\n",
    "    # Ratio of weeks with observation over prior N weeks\n",
    "    df[f\"obs_ratio_prior_{prior_weeks_obs_ratio_window}w\"] = df.groupby(h3_col)[\n",
    "        obs_flag_col\n",
    "    ].transform(lambda x: x.shift(1).rolling(prior_weeks_obs_ratio_window).mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_parent_h3_features(\n",
    "    df,\n",
    "    h3_parent_cols=[\"H3_GRID_PARENT_1\", \"H3_GRID_PARENT_2\", \"H3_GRID_PARENT_3\"],\n",
    "    target_col=\"SIGHTING_RATIO\",\n",
    "    time_col=\"WEEK_START\",\n",
    "    lag_weeks=[1, 2, 3, 4, 5, 28, 29, 52, 56, 57, 113],\n",
    "    rolling_windows=[3, 5],\n",
    "    add_presence_booleans=True,\n",
    "    add_rolling_means=True,\n",
    "):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    for parent_col in h3_parent_cols:\n",
    "        parent_level = parent_col.split(\"_\")[-1].lower()  # e.g. '1', '2', '3'\n",
    "        group_cols = [parent_col, time_col]\n",
    "\n",
    "        # Step 1: Aggregate child-level target to parent grid by week\n",
    "        agg = (\n",
    "            df.groupby(group_cols)[target_col]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "            .rename(columns={target_col: f\"{target_col}_sum_parent_{parent_level}\"})\n",
    "        )\n",
    "\n",
    "        # Step 2: Add lags\n",
    "        agg = agg.sort_values(by=[parent_col, time_col])\n",
    "        for lag in lag_weeks:\n",
    "            agg[f\"{target_col}_sum_parent_{parent_level}_lag_{lag}\"] = (\n",
    "                agg.groupby(parent_col)[f\"{target_col}_sum_parent_{parent_level}\"]\n",
    "                .shift(lag)\n",
    "                .fillna(0)\n",
    "            )\n",
    "\n",
    "        # Step 3: Optionally add rolling means\n",
    "        if add_rolling_means:\n",
    "            for win in rolling_windows:\n",
    "                agg[f\"{target_col}_sum_parent_{parent_level}_rollmean_{win}\"] = (\n",
    "                    agg.groupby(parent_col)[f\"{target_col}_sum_parent_{parent_level}\"]\n",
    "                    .rolling(window=win, min_periods=1)\n",
    "                    .mean()\n",
    "                    .reset_index(level=0, drop=True)\n",
    "                )\n",
    "\n",
    "        # Step 4: Optionally add presence boolean\n",
    "        if add_presence_booleans:\n",
    "            for lag in lag_weeks:\n",
    "                agg[f\"presence_parent_{parent_level}_lag_{lag}\"] = (\n",
    "                    agg[f\"{target_col}_sum_parent_{parent_level}_lag_{lag}\"] > 0\n",
    "                ).astype(int)\n",
    "\n",
    "        # Step 5: Merge back to original df\n",
    "        df = df.merge(\n",
    "            agg,\n",
    "            how=\"left\",\n",
    "            left_on=[parent_col, time_col],\n",
    "            right_on=[parent_col, time_col],\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9604e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Copy Raw Data\n",
    "sightings_data = sightings_data_raw.copy()\n",
    "\n",
    "# 1. Process Weekly Sightings\n",
    "sightings_data = process_weekly_sightings(sightings_data, h3_resolution)\n",
    "\n",
    "# 2. Add Lagged Features\n",
    "sightings_data = add_lag_features_ratio_base(\n",
    "    sightings_data, h3_col=f\"H3_GRID_{h3_resolution}\"\n",
    ")\n",
    "\n",
    "# 3. Add Parent Grid Information\n",
    "sightings_data = add_parent_h3_features(sightings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f9a1e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - Percent Grid Over Water (Use Major Shorelines from WSDOT, Need to Find it for CA)\n",
    "# - Data Transition Week (e.g. TWM -> Acartia)??\n",
    "\n",
    "# - Neighbor Lagged Sightings\n",
    "# - Neighbor Prescence Lagged Sightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934235f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorically Encode H3 Grid - One-Hot Encoding might help xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dd033108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sightings_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13555b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
